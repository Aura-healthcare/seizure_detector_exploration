{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofr72LxX5abr",
        "colab_type": "code",
        "outputId": "c3c07b22-6927-428c-8d0b-2efa373d127c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import json\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 100)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfrKFGSu5iTc",
        "colab_type": "code",
        "outputId": "0ecd0e9e-1abc-4aba-c9ba-a73fb38837a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "source": [
        "xqrs = pd.read_csv('/home/zozi/Documents/Code/Projects/Epilepsy_Project/Aura/to_csv/final_dataset/xqrs_filtered.csv')\n",
        "xqrs.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                         Key  hf  lf_hf_ratio  vlf  \\\n0  train_01_tcp_ar_7793_s001_2011_05_27_t001 NaN          NaN  NaN   \n1  train_01_tcp_ar_7793_s001_2011_05_27_t001 NaN          NaN  NaN   \n2  train_01_tcp_ar_7793_s001_2011_05_27_t001 NaN          NaN  NaN   \n3  train_01_tcp_ar_7793_s001_2011_05_27_t001 NaN          NaN  NaN   \n4  train_01_tcp_ar_7793_s001_2011_05_27_t001 NaN          NaN  NaN   \n\n       max_hr  lf  cvi  sd1      min_hr  sd2  sampen  median_nni     mean_hr  \\\n0  127.118644 NaN  NaN  NaN  105.633803  NaN     NaN       506.0  118.490493   \n1  120.000000 NaN  NaN  NaN  118.110236  NaN     NaN       504.0  119.049198   \n2  120.967742 NaN  NaN  NaN  118.110236  NaN     NaN       502.0  119.532917   \n3  120.967742 NaN  NaN  NaN  116.279070  NaN     NaN       512.0  117.713266   \n4  120.967742 NaN  NaN  NaN  118.110236  NaN     NaN       504.0  118.816291   \n\n   mean_nni  csi  Modified_csi  label  \n0     507.8  NaN           NaN  0.000  \n1     504.0  NaN           NaN  0.016  \n2     502.0  NaN           NaN  1.000  \n3     509.8  NaN           NaN  1.000  \n4     505.0  NaN           NaN  1.000  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Key</th>\n      <th>hf</th>\n      <th>lf_hf_ratio</th>\n      <th>vlf</th>\n      <th>max_hr</th>\n      <th>lf</th>\n      <th>cvi</th>\n      <th>sd1</th>\n      <th>min_hr</th>\n      <th>sd2</th>\n      <th>sampen</th>\n      <th>median_nni</th>\n      <th>mean_hr</th>\n      <th>mean_nni</th>\n      <th>csi</th>\n      <th>Modified_csi</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_01_tcp_ar_7793_s001_2011_05_27_t001</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>127.118644</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>105.633803</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>506.0</td>\n      <td>118.490493</td>\n      <td>507.8</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_01_tcp_ar_7793_s001_2011_05_27_t001</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>120.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>118.110236</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>504.0</td>\n      <td>119.049198</td>\n      <td>504.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.016</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_01_tcp_ar_7793_s001_2011_05_27_t001</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>120.967742</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>118.110236</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>502.0</td>\n      <td>119.532917</td>\n      <td>502.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_01_tcp_ar_7793_s001_2011_05_27_t001</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>120.967742</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>116.279070</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>512.0</td>\n      <td>117.713266</td>\n      <td>509.8</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_01_tcp_ar_7793_s001_2011_05_27_t001</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>120.967742</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>118.110236</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>504.0</td>\n      <td>118.816291</td>\n      <td>505.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm7dsfixWdLD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e282245b-4eb6-43aa-d53e-18a98d60c05a"
      },
      "source": [
        "xqrs.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "(51010, 17)"
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTNdVmCCWsMx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0897754-bfbe-405f-a898-44262d2625e0"
      },
      "source": [
        "xqrs[xqrs.Key.str.contains('9578')].shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "(11524, 17)"
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o4Ncb3gXmr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xqrs.dropna(subset=['label'], inplace=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7XNV8l_XBNF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "a694f821-017e-498a-f4a4-e062d286ed3e"
      },
      "source": [
        "xqrs[xqrs.Key.str.contains('9578')].describe()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "                hf  lf_hf_ratio          vlf        max_hr           lf  \\\ncount  8105.000000  8105.000000  8105.000000  1.136400e+04  8105.000000   \nmean     13.579335    16.331932   125.043774           inf   158.839724   \nstd      31.865895    14.964438   246.466942           NaN   319.918882   \nmin       0.402726     0.714182     0.196191  6.493506e+01     1.186754   \n25%       2.505402     6.207651    18.315336  8.827586e+01    29.173682   \n50%       5.471209    12.437859    50.651091  9.423313e+01    72.284240   \n75%      12.612775    20.967575   124.508170  1.017219e+02   171.102855   \nmax    1023.599070   147.445933  3377.910241           inf  7108.100125   \n\n               cvi          sd1        min_hr          sd2       sampen  \\\ncount  9936.000000  9936.000000  1.136400e+04  9936.000000  9960.000000   \nmean      3.057056     4.660153           inf    23.290267          inf   \nstd       0.513955     3.227171           NaN    17.545981          NaN   \nmin       1.470822     0.801546  5.976654e+01     2.059374     0.063899   \n25%       2.709756     2.508850  8.393443e+01    11.630395     0.493014   \n50%       3.053606     3.745094  9.035294e+01    18.324940     0.681686   \n75%       3.404144     5.851221  9.677419e+01    29.577130     0.910156   \nmax       4.978590    47.205406           inf   152.819491          inf   \n\n         median_nni       mean_hr      mean_nni          csi  Modified_csi  \\\ncount  11350.000000  1.135000e+04  11350.000000  9936.000000   9936.000000   \nmean     657.411967           inf    656.673516     5.432534    609.768193   \nstd       72.883216           NaN     72.548627     3.014876    807.978881   \nmin        0.000000  6.270795e+01      0.000000     0.771190     12.482404   \n25%      609.375000  8.557179e+01    608.035817     3.457654    167.015925   \n50%      652.343750  9.187811e+01    653.125000     4.740496    351.641642   \n75%      703.125000  9.873845e+01    701.450893     6.603845    732.311680   \nmax      960.000000           inf    957.454545    36.710563  15007.698911   \n\n              label  \ncount  11374.000000  \nmean       0.115033  \nstd        0.317778  \nmin        0.000000  \n25%        0.000000  \n50%        0.000000  \n75%        0.000000  \nmax        1.000000  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hf</th>\n      <th>lf_hf_ratio</th>\n      <th>vlf</th>\n      <th>max_hr</th>\n      <th>lf</th>\n      <th>cvi</th>\n      <th>sd1</th>\n      <th>min_hr</th>\n      <th>sd2</th>\n      <th>sampen</th>\n      <th>median_nni</th>\n      <th>mean_hr</th>\n      <th>mean_nni</th>\n      <th>csi</th>\n      <th>Modified_csi</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>8105.000000</td>\n      <td>8105.000000</td>\n      <td>8105.000000</td>\n      <td>1.136400e+04</td>\n      <td>8105.000000</td>\n      <td>9936.000000</td>\n      <td>9936.000000</td>\n      <td>1.136400e+04</td>\n      <td>9936.000000</td>\n      <td>9960.000000</td>\n      <td>11350.000000</td>\n      <td>1.135000e+04</td>\n      <td>11350.000000</td>\n      <td>9936.000000</td>\n      <td>9936.000000</td>\n      <td>11374.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>13.579335</td>\n      <td>16.331932</td>\n      <td>125.043774</td>\n      <td>inf</td>\n      <td>158.839724</td>\n      <td>3.057056</td>\n      <td>4.660153</td>\n      <td>inf</td>\n      <td>23.290267</td>\n      <td>inf</td>\n      <td>657.411967</td>\n      <td>inf</td>\n      <td>656.673516</td>\n      <td>5.432534</td>\n      <td>609.768193</td>\n      <td>0.115033</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>31.865895</td>\n      <td>14.964438</td>\n      <td>246.466942</td>\n      <td>NaN</td>\n      <td>319.918882</td>\n      <td>0.513955</td>\n      <td>3.227171</td>\n      <td>NaN</td>\n      <td>17.545981</td>\n      <td>NaN</td>\n      <td>72.883216</td>\n      <td>NaN</td>\n      <td>72.548627</td>\n      <td>3.014876</td>\n      <td>807.978881</td>\n      <td>0.317778</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.402726</td>\n      <td>0.714182</td>\n      <td>0.196191</td>\n      <td>6.493506e+01</td>\n      <td>1.186754</td>\n      <td>1.470822</td>\n      <td>0.801546</td>\n      <td>5.976654e+01</td>\n      <td>2.059374</td>\n      <td>0.063899</td>\n      <td>0.000000</td>\n      <td>6.270795e+01</td>\n      <td>0.000000</td>\n      <td>0.771190</td>\n      <td>12.482404</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2.505402</td>\n      <td>6.207651</td>\n      <td>18.315336</td>\n      <td>8.827586e+01</td>\n      <td>29.173682</td>\n      <td>2.709756</td>\n      <td>2.508850</td>\n      <td>8.393443e+01</td>\n      <td>11.630395</td>\n      <td>0.493014</td>\n      <td>609.375000</td>\n      <td>8.557179e+01</td>\n      <td>608.035817</td>\n      <td>3.457654</td>\n      <td>167.015925</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.471209</td>\n      <td>12.437859</td>\n      <td>50.651091</td>\n      <td>9.423313e+01</td>\n      <td>72.284240</td>\n      <td>3.053606</td>\n      <td>3.745094</td>\n      <td>9.035294e+01</td>\n      <td>18.324940</td>\n      <td>0.681686</td>\n      <td>652.343750</td>\n      <td>9.187811e+01</td>\n      <td>653.125000</td>\n      <td>4.740496</td>\n      <td>351.641642</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>12.612775</td>\n      <td>20.967575</td>\n      <td>124.508170</td>\n      <td>1.017219e+02</td>\n      <td>171.102855</td>\n      <td>3.404144</td>\n      <td>5.851221</td>\n      <td>9.677419e+01</td>\n      <td>29.577130</td>\n      <td>0.910156</td>\n      <td>703.125000</td>\n      <td>9.873845e+01</td>\n      <td>701.450893</td>\n      <td>6.603845</td>\n      <td>732.311680</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1023.599070</td>\n      <td>147.445933</td>\n      <td>3377.910241</td>\n      <td>inf</td>\n      <td>7108.100125</td>\n      <td>4.978590</td>\n      <td>47.205406</td>\n      <td>inf</td>\n      <td>152.819491</td>\n      <td>inf</td>\n      <td>960.000000</td>\n      <td>inf</td>\n      <td>957.454545</td>\n      <td>36.710563</td>\n      <td>15007.698911</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgJYxSyN6a52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xqrs_filtered = xqrs.replace([np.inf, -np.inf], np.nan)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itDRugAU6g51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xqrs_clean_imputed = xqrs_filtered.groupby(xqrs_filtered['Key']).transform(lambda x: x.fillna(x.mean()))\n",
        "xqrs_clean_imputed.insert(0, 'Key', xqrs_filtered.Key)\n",
        "xqrs_clean_imputed = xqrs_clean_imputed.reset_index().drop(['index'], axis=1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "                 hf   lf_hf_ratio           vlf        max_hr            lf  \\\ncount  11374.000000  11374.000000  11374.000000  11374.000000  11374.000000   \nmean      13.011481     16.040017    119.131736     95.547740    152.370690   \nstd       28.700576     14.093807    220.137625     11.140364    286.539398   \nmin        0.402726      0.714182      0.196191     64.935065      1.186754   \n25%        2.620854      6.345730     18.790028     88.275862     30.173069   \n50%        5.497420     12.725140     53.299327     94.233129     72.679104   \n75%       12.013642     20.571329    126.216635    101.721854    162.940279   \nmax     1023.599070    147.445933   3377.910241    199.480519   7108.100125   \n\n                cvi           sd1        min_hr           sd2        sampen  \\\ncount  11374.000000  11374.000000  11374.000000  11374.000000  11374.000000   \nmean       3.054679      4.640596     90.607419     23.215062      0.726211   \nstd        0.505772      3.153361     10.181178     17.010099      0.322632   \nmin        1.470822      0.801546     59.766537      2.059374      0.063899   \n25%        2.723533      2.537167     83.934426     11.811989      0.505232   \n50%        3.049807      3.749115     90.352941     18.675215      0.684323   \n75%        3.388447      5.777824     96.774194     29.362322      0.897739   \nmax        4.978590     47.205406    142.883721    152.819491      3.433987   \n\n         median_nni       mean_hr      mean_nni           csi  Modified_csi  \\\ncount  11374.000000  11374.000000  11374.000000  11374.000000  11374.000000   \nmean     657.588218     92.473435    656.849432      5.428086    606.813985   \nstd       72.976056     10.205161     72.642005      2.906799    770.938494   \nmin        0.000000     62.707951      0.000000      0.771190     12.482404   \n25%      609.375000     85.548888    608.228401      3.518052    171.091760   \n50%      652.343750     91.869203    653.125000      4.792521    370.774514   \n75%      703.125000     98.696809    701.726004      6.632810    751.111647   \nmax      960.000000    160.675088    957.454545     36.710563  15007.698911   \n\n              label  \ncount  11374.000000  \nmean       0.115033  \nstd        0.317778  \nmin        0.000000  \n25%        0.000000  \n50%        0.000000  \n75%        0.000000  \nmax        1.000000  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hf</th>\n      <th>lf_hf_ratio</th>\n      <th>vlf</th>\n      <th>max_hr</th>\n      <th>lf</th>\n      <th>cvi</th>\n      <th>sd1</th>\n      <th>min_hr</th>\n      <th>sd2</th>\n      <th>sampen</th>\n      <th>median_nni</th>\n      <th>mean_hr</th>\n      <th>mean_nni</th>\n      <th>csi</th>\n      <th>Modified_csi</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>11374.000000</td>\n      <td>11374.000000</td>\n      <td>11374.000000</td>\n      <td>11374.000000</td>\n      <td>11374.000000</td>\n      <td>11374.000000</td>\n      <td>11374.000000</td>\n      <td>11374.000000</td>\n      <td>11374.000000</td>\n      <td>11374.000000</td>\n      <td>11374.000000</td>\n      <td>11374.000000</td>\n      <td>11374.000000</td>\n      <td>11374.000000</td>\n      <td>11374.000000</td>\n      <td>11374.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>13.011481</td>\n      <td>16.040017</td>\n      <td>119.131736</td>\n      <td>95.547740</td>\n      <td>152.370690</td>\n      <td>3.054679</td>\n      <td>4.640596</td>\n      <td>90.607419</td>\n      <td>23.215062</td>\n      <td>0.726211</td>\n      <td>657.588218</td>\n      <td>92.473435</td>\n      <td>656.849432</td>\n      <td>5.428086</td>\n      <td>606.813985</td>\n      <td>0.115033</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>28.700576</td>\n      <td>14.093807</td>\n      <td>220.137625</td>\n      <td>11.140364</td>\n      <td>286.539398</td>\n      <td>0.505772</td>\n      <td>3.153361</td>\n      <td>10.181178</td>\n      <td>17.010099</td>\n      <td>0.322632</td>\n      <td>72.976056</td>\n      <td>10.205161</td>\n      <td>72.642005</td>\n      <td>2.906799</td>\n      <td>770.938494</td>\n      <td>0.317778</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.402726</td>\n      <td>0.714182</td>\n      <td>0.196191</td>\n      <td>64.935065</td>\n      <td>1.186754</td>\n      <td>1.470822</td>\n      <td>0.801546</td>\n      <td>59.766537</td>\n      <td>2.059374</td>\n      <td>0.063899</td>\n      <td>0.000000</td>\n      <td>62.707951</td>\n      <td>0.000000</td>\n      <td>0.771190</td>\n      <td>12.482404</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2.620854</td>\n      <td>6.345730</td>\n      <td>18.790028</td>\n      <td>88.275862</td>\n      <td>30.173069</td>\n      <td>2.723533</td>\n      <td>2.537167</td>\n      <td>83.934426</td>\n      <td>11.811989</td>\n      <td>0.505232</td>\n      <td>609.375000</td>\n      <td>85.548888</td>\n      <td>608.228401</td>\n      <td>3.518052</td>\n      <td>171.091760</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.497420</td>\n      <td>12.725140</td>\n      <td>53.299327</td>\n      <td>94.233129</td>\n      <td>72.679104</td>\n      <td>3.049807</td>\n      <td>3.749115</td>\n      <td>90.352941</td>\n      <td>18.675215</td>\n      <td>0.684323</td>\n      <td>652.343750</td>\n      <td>91.869203</td>\n      <td>653.125000</td>\n      <td>4.792521</td>\n      <td>370.774514</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>12.013642</td>\n      <td>20.571329</td>\n      <td>126.216635</td>\n      <td>101.721854</td>\n      <td>162.940279</td>\n      <td>3.388447</td>\n      <td>5.777824</td>\n      <td>96.774194</td>\n      <td>29.362322</td>\n      <td>0.897739</td>\n      <td>703.125000</td>\n      <td>98.696809</td>\n      <td>701.726004</td>\n      <td>6.632810</td>\n      <td>751.111647</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1023.599070</td>\n      <td>147.445933</td>\n      <td>3377.910241</td>\n      <td>199.480519</td>\n      <td>7108.100125</td>\n      <td>4.978590</td>\n      <td>47.205406</td>\n      <td>142.883721</td>\n      <td>152.819491</td>\n      <td>3.433987</td>\n      <td>960.000000</td>\n      <td>160.675088</td>\n      <td>957.454545</td>\n      <td>36.710563</td>\n      <td>15007.698911</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "xqrs_clean_imputed[xqrs_clean_imputed.Key.str.contains('9578')].describe()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuvMkOuz6ipR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xqrs_clean_imputed.label = xqrs_clean_imputed.label.apply(lambda x: 1 if x != 0 else 0)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrxFkLVc6kLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from functions import supervised_by_exam"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Uw3VAE-6y5q",
        "colab_type": "code",
        "outputId": "77a550aa-ae32-441c-f943-cd1efec7feca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "xqrs_clean_lag_10 = supervised_by_exam(xqrs_clean_imputed)\n",
        "xqrs_clean_lag_30 = supervised_by_exam(xqrs_clean_imputed, 3)\n",
        "xqrs_clean_lag_60 = supervised_by_exam(xqrs_clean_imputed, 6)\n",
        "xqrs_clean_lag_120 = supervised_by_exam(xqrs_clean_imputed, 12)\n",
        "xqrs_clean_lag_240 = supervised_by_exam(xqrs_clean_imputed, 24)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "CPU times: user 1min 34s, sys: 13.1 s, total: 1min 47s\nWall time: 1min 47s\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlcgwJRWAKTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def reg(text):\n",
        "    r = re.search(r'_(\\d+)_s', text)\n",
        "    if r:\n",
        "        return r.group(1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO6yRyyfAT9Q",
        "colab_type": "code",
        "outputId": "98382efb-f9c5-4665-cd85-6ea4709d1d69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "patients = set(xqrs_clean_imputed.Key.apply(reg))\n",
        "len(patients)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "59"
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2LLwchq7uEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6JW-es0BSwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_df = {\n",
        "    'normal': xqrs_clean_imputed,\n",
        "    '10': xqrs_clean_lag_10,\n",
        "    '30': xqrs_clean_lag_30,\n",
        "    '60': xqrs_clean_lag_60,\n",
        "    '120': xqrs_clean_lag_120,\n",
        "    '240': xqrs_clean_lag_240\n",
        "}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5zna4UXCflj",
        "colab_type": "code",
        "outputId": "2713bee4-e133-4cde-b29c-7ea2ccff133f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "for df in dict_df:\n",
        "  print(df)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "normal\n10\n30\n60\n120\n240\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkGvVbL2Pu06",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "72f6abce-1061-4a00-cf99-d2f74ae2dab4"
      },
      "source": [
        "dict_df['60'][dict_df['60'].Key.str.contains('9578')].describe(include='all')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "           NaN           NaN           NaN           NaN   \nmean           16.161888    121.553360     95.616836    154.608713   \nstd            14.114258    227.118045     11.260092    295.489349   \nmin             0.714182      0.196191     67.873303      1.186754   \n25%             6.403305     19.650680     88.275862     31.134530   \n50%            12.930494     54.882191     94.233129     73.608850   \n75%            20.705437    129.374120    101.721854    163.894554   \nmax           147.445933   3377.910241    199.480519   7108.100125   \n\n            cvi(t-6)      sd1(t-6)   min_hr(t-6)      sd2(t-6)   sampen(t-6)  \\\ncount   10162.000000  10162.000000  10162.000000  10162.000000  10162.000000   \nunique           NaN           NaN           NaN           NaN           NaN   \ntop              NaN           NaN           NaN           NaN           NaN   \nfreq             NaN           NaN           NaN           NaN           NaN   \nmean        3.058257      4.660172     90.652119     23.405456      0.725931   \nstd         0.506574      3.199092     10.244034     17.250134      0.325818   \nmin         1.470822      0.801546     59.766537      2.059374      0.063899   \n25%         2.724546      2.542668     83.934426     11.881367      0.502114   \n50%         3.051493      3.743001     90.352941     18.752836      0.682382   \n75%         3.394944      5.823403     97.215190     29.604736      0.897739   \nmax         4.978590     47.205406    142.883721    152.819491      3.433987   \n\n        median_nni(t-6)  mean_hr(t-6)  mean_nni(t-6)      csi(t-6)  \\\ncount      10162.000000  10162.000000   10162.000000  10162.000000   \nunique              NaN           NaN            NaN           NaN   \ntop                 NaN           NaN            NaN           NaN   \nfreq                NaN           NaN            NaN           NaN   \nmean         657.309157     92.525921     656.565873      5.458394   \nstd           73.456321     10.276343      73.131143      2.926033   \nmin            0.000000     65.806031       0.000000      0.771190   \n25%          609.375000     85.558472     607.177734      3.532118   \n50%          652.343750     91.957896     652.604167      4.820640   \n75%          703.125000     98.867548     701.450893      6.662837   \nmax          928.000000    160.675088     912.642045     36.710563   \n\n        Modified_csi(t-6)       hf(t-5)  lf_hf_ratio(t-5)      vlf(t-5)  \\\ncount        10162.000000  10162.000000      10162.000000  10162.000000   \nunique                NaN           NaN               NaN           NaN   \ntop                   NaN           NaN               NaN           NaN   \nfreq                  NaN           NaN               NaN           NaN   \nmean           616.210998     13.202052         16.150197    121.413257   \nstd            786.683205     29.613253         14.119956    227.141263   \nmin             12.482404      0.402726          0.714182      0.196191   \n25%            172.551041      2.646750          6.403305     19.471676   \n50%            377.828466      5.610511         12.867016     54.727040   \n75%            760.727809     12.013642         20.705437    129.374120   \nmax          15007.698911   1023.599070        147.445933   3377.910241   \n\n         max_hr(t-5)       lf(t-5)      cvi(t-5)      sd1(t-5)   min_hr(t-5)  \\\ncount   10162.000000  10162.000000  10162.000000  10162.000000  10162.000000   \nunique           NaN           NaN           NaN           NaN           NaN   \ntop              NaN           NaN           NaN           NaN           NaN   \nfreq             NaN           NaN           NaN           NaN           NaN   \nmean       95.591724    154.694490      3.057858      4.659211     90.642698   \nstd        11.211105    295.687997      0.507516      3.203542     10.225034   \nmin        67.873303      1.186754      1.470822      0.801546     59.766537   \n25%        88.275862     30.826074      2.722456      2.537167     83.934426   \n50%        94.233129     73.426403      3.050966      3.737422     90.352941   \n75%       101.721854    164.134068      3.395798      5.826257     97.215190   \nmax       199.480519   7108.100125      4.978590     47.205406    142.222222   \n\n            sd2(t-5)   sampen(t-5)  median_nni(t-5)  mean_hr(t-5)  \\\ncount   10162.000000  10162.000000     10162.000000  10162.000000   \nunique           NaN           NaN              NaN           NaN   \ntop              NaN           NaN              NaN           NaN   \nfreq             NaN           NaN              NaN           NaN   \nmean       23.379304      0.726619       657.339153     92.518562   \nstd        17.276516      0.327494        73.427224     10.273398   \nmin         2.059374      0.063899         0.000000     65.806031   \n25%        11.829085      0.501123       609.375000     85.552784   \n50%        18.705508      0.682492       652.343750     91.953200   \n75%        29.592565      0.900663       703.125000     98.819902   \nmax       152.819491      3.433987       928.000000    160.675088   \n\n        mean_nni(t-5)      csi(t-5)  Modified_csi(t-5)       hf(t-4)  \\\ncount    10162.000000  10162.000000       10162.000000  10162.000000   \nunique            NaN           NaN                NaN           NaN   \ntop               NaN           NaN                NaN           NaN   \nfreq              NaN           NaN                NaN           NaN   \nmean       656.610956      5.453695         614.718494     13.193538   \nstd         73.110828      2.934942         788.101950     29.599020   \nmin          0.000000      0.771190          12.482404      0.402726   \n25%        607.421875      3.525008         171.960201      2.634705   \n50%        652.604167      4.807779         376.104969      5.608777   \n75%        701.676339      6.656457         756.185376     12.013642   \nmax        912.642045     36.710563       15007.698911   1023.599070   \n\n        lf_hf_ratio(t-4)      vlf(t-4)   max_hr(t-4)       lf(t-4)  \\\ncount       10162.000000  10162.000000  10162.000000  10162.000000   \nunique               NaN           NaN           NaN           NaN   \ntop                  NaN           NaN           NaN           NaN   \nfreq                 NaN           NaN           NaN           NaN   \nmean           16.133862    121.245740     95.585314    154.458557   \nstd            14.129624    227.193073     11.217543    295.479418   \nmin             0.714182      0.196191     67.873303      1.186754   \n25%             6.384462     19.264986     88.275862     30.655258   \n50%            12.820092     54.672552     94.233129     73.100181   \n75%            20.703865    128.166316    101.721854    164.435367   \nmax           147.445933   3377.910241    199.480519   7108.100125   \n\n            cvi(t-4)      sd1(t-4)   min_hr(t-4)      sd2(t-4)   sampen(t-4)  \\\ncount   10162.000000  10162.000000  10162.000000  10162.000000  10162.000000   \nunique           NaN           NaN           NaN           NaN           NaN   \ntop              NaN           NaN           NaN           NaN           NaN   \nfreq             NaN           NaN           NaN           NaN           NaN   \nmean        3.057768      4.659560     90.637588     23.358871      0.727097   \nstd         0.508284      3.204942     10.230340     17.309044      0.329171   \nmin         1.470822      0.801546     59.766537      2.059374      0.063899   \n25%         2.721479      2.532909     83.934426     11.807143      0.499905   \n50%         3.050709      3.738209     90.352941     18.605535      0.682639   \n75%         3.396226      5.835041     97.215190     29.592565      0.902868   \nmax         4.978590     47.205406    142.222222    152.819491      3.433987   \n\n        median_nni(t-4)  mean_hr(t-4)  mean_nni(t-4)      csi(t-4)  \\\ncount      10162.000000  10162.000000   10162.000000  10162.000000   \nunique              NaN           NaN            NaN           NaN   \ntop                 NaN           NaN            NaN           NaN   \nfreq                NaN           NaN            NaN           NaN   \nmean         657.452077     92.513347     656.723477      5.448779   \nstd           73.174669     10.276739      72.854548      2.947190   \nmin            0.000000     65.806031       0.000000      0.771190   \n25%          609.375000     85.539833     607.421875      3.512167   \n50%          652.343750     91.953200     652.604167      4.791672   \n75%          703.125000     98.819902     701.729911      6.648563   \nmax          928.000000    160.675088     912.642045     36.710563   \n\n        Modified_csi(t-4)       hf(t-3)  lf_hf_ratio(t-3)      vlf(t-3)  \\\ncount        10162.000000  10162.000000      10162.000000  10162.000000   \nunique                NaN           NaN               NaN           NaN   \ntop                   NaN           NaN               NaN           NaN   \nfreq                  NaN           NaN               NaN           NaN   \nmean           613.607499     13.208245         16.126224    121.238918   \nstd            790.447446     29.638782         14.155953    227.468660   \nmin             12.482404      0.402726          0.714182      0.196191   \n25%            171.114292      2.629791          6.369572     19.126927   \n50%            370.879433      5.604066         12.781229     54.248020   \n75%            752.950486     12.041350         20.683849    127.352379   \nmax          15007.698911   1023.599070        147.445933   3377.910241   \n\n         max_hr(t-3)  ...  mean_hr(t-3)  mean_nni(t-3)      csi(t-3)  \\\ncount   10162.000000  ...  10162.000000   10162.000000  10162.000000   \nunique           NaN  ...           NaN            NaN           NaN   \ntop              NaN  ...           NaN            NaN           NaN   \nfreq             NaN  ...           NaN            NaN           NaN   \nmean       95.574190  ...     92.503103     656.800012      5.444579   \nstd        11.218700  ...     10.277674      72.873766      2.962006   \nmin        65.502183  ...     63.546436       0.000000      0.771190   \n25%        88.275862  ...     85.526079     607.536765      3.498556   \n50%        94.233129  ...     91.948182     652.604167      4.782166   \n75%       101.721854  ...     98.817499     701.729911      6.641931   \nmax       199.480519  ...    160.675088     944.363636     36.710563   \n\n        Modified_csi(t-3)       hf(t-2)  lf_hf_ratio(t-2)      vlf(t-2)  \\\ncount        10162.000000  10162.000000      10162.000000  10162.000000   \nunique                NaN           NaN               NaN           NaN   \ntop                   NaN           NaN               NaN           NaN   \nfreq                  NaN           NaN               NaN           NaN   \nmean           612.856547     13.211123         16.115817    121.192930   \nstd            794.069903     29.646893         14.191359    227.788720   \nmin             12.482404      0.402726          0.714182      0.196191   \n25%            170.088308      2.620090          6.346912     19.033227   \n50%            366.736460      5.574274         12.761474     53.561327   \n75%            748.453376     12.096541         20.670866    126.839991   \nmax          15007.698911   1023.599070        147.445933   3377.910241   \n\n         max_hr(t-2)       lf(t-2)      cvi(t-2)      sd1(t-2)   min_hr(t-2)  \\\ncount   10162.000000  10162.000000  10162.000000  10162.000000  10162.000000   \nunique           NaN           NaN           NaN           NaN           NaN   \ntop              NaN           NaN           NaN           NaN           NaN   \nfreq             NaN           NaN           NaN           NaN           NaN   \nmean       95.564185    154.461406      3.057695      4.661256     90.616864   \nstd        11.215230    295.935120      0.510509      3.210132     10.237693   \nmin        65.502183      1.186754      1.470822      0.801546     59.766537   \n25%        88.275862     30.372360      2.716477      2.525518     83.934426   \n50%        94.233129     72.859567      3.052602      3.744269     90.352941   \n75%       101.721854    165.969172      3.399983      5.839973     97.215190   \nmax       199.480519   7108.100125      4.978590     47.205406    142.222222   \n\n            sd2(t-2)   sampen(t-2)  median_nni(t-2)  mean_hr(t-2)  \\\ncount   10162.000000  10162.000000     10162.000000  10162.000000   \nunique           NaN           NaN              NaN           NaN   \ntop              NaN           NaN              NaN           NaN   \nfreq             NaN           NaN              NaN           NaN   \nmean       23.333946      0.727117       657.630550     92.492177   \nstd        17.392679      0.332323        73.279488     10.282690   \nmin         2.059374      0.063899         0.000000     63.546436   \n25%        11.742268      0.497914       609.375000     85.512200   \n50%        18.493598      0.681751       652.343750     91.953972   \n75%        29.656429      0.904662       703.125000     98.779926   \nmax       152.819491      3.433987       944.000000    160.675088   \n\n        mean_nni(t-2)      csi(t-2)  Modified_csi(t-2)       hf(t-1)  \\\ncount    10162.000000  10162.000000       10162.000000  10162.000000   \nunique            NaN           NaN                NaN           NaN   \ntop               NaN           NaN                NaN           NaN   \nfreq              NaN           NaN                NaN           NaN   \nmean       656.889975      5.440220         612.221739     13.240601   \nstd         72.938276      2.976300         797.852625     29.730434   \nmin          0.000000      0.771190          12.482404      0.402726   \n25%        607.666016      3.489128         169.442786      2.615222   \n50%        652.604167      4.763840         363.567283      5.552642   \n75%        701.822917      6.628589         744.020490     12.147710   \nmax        944.363636     36.710563       15007.698911   1023.599070   \n\n        lf_hf_ratio(t-1)      vlf(t-1)   max_hr(t-1)       lf(t-1)  \\\ncount       10162.000000  10162.000000  10162.000000  10162.000000   \nunique               NaN           NaN           NaN           NaN   \ntop                  NaN           NaN           NaN           NaN   \nfreq                 NaN           NaN           NaN           NaN   \nmean           16.117698    121.244891     95.559541    154.621150   \nstd            14.261396    228.172750     11.224031    296.474584   \nmin             0.714182      0.196191     64.935065      1.186754   \n25%             6.345730     18.891034     88.275862     30.188127   \n50%            12.717142     53.085749     94.233129     72.859567   \n75%            20.670103    126.625348    101.721854    166.612037   \nmax           147.445933   3377.910241    199.480519   7108.100125   \n\n            cvi(t-1)      sd1(t-1)   min_hr(t-1)      sd2(t-1)   sampen(t-1)  \\\ncount   10162.000000  10162.000000  10162.000000  10162.000000  10162.000000   \nunique           NaN           NaN           NaN           NaN           NaN   \ntop              NaN           NaN           NaN           NaN           NaN   \nfreq             NaN           NaN           NaN           NaN           NaN   \nmean        3.057848      4.664603     90.610947     23.326801      0.727365   \nstd         0.511871      3.217505     10.250864     17.440551      0.333284   \nmin         1.470822      0.801546     59.766537      2.059374      0.063899   \n25%         2.713910      2.518875     83.934426     11.710161      0.496647   \n50%         3.053419      3.746811     90.352941     18.452765      0.681620   \n75%         3.401522      5.844929     97.215190     29.656429      0.905322   \nmax         4.978590     47.205406    142.222222    152.819491      3.433987   \n\n        median_nni(t-1)  mean_hr(t-1)  mean_nni(t-1)      csi(t-1)  \\\ncount      10162.000000  10162.000000   10162.000000  10162.000000   \nunique              NaN           NaN            NaN           NaN   \ntop                 NaN           NaN            NaN           NaN   \nfreq                NaN           NaN            NaN           NaN   \nmean         657.791762     92.484198     657.042332      5.434736   \nstd           73.095392     10.292418      72.760950      2.984035   \nmin          371.093750     62.707951     382.412997      0.771190   \n25%          609.375000     85.504068     607.802447      3.479076   \n50%          652.343750     91.944863     652.604167      4.753710   \n75%          703.125000     98.765365     702.006696      6.617435   \nmax          960.000000    160.675088     957.454545     36.710563   \n\n        Modified_csi(t-1)         hf(t)  lf_hf_ratio(t)        vlf(t)  \\\ncount        10162.000000  10162.000000    10162.000000  10162.000000   \nunique                NaN           NaN             NaN           NaN   \ntop                   NaN           NaN             NaN           NaN   \nfreq                  NaN           NaN             NaN           NaN   \nmean           611.028277     13.255576       16.117865    121.038902   \nstd            799.596247     29.773111       14.353969    228.275105   \nmin             12.482404      0.402726        0.714182      0.196191   \n25%            168.624719      2.607297        6.315848     18.670768   \n50%            359.297258      5.497420       12.665044     52.512485   \n75%            740.018576     12.190304       20.670103    126.403313   \nmax          15007.698911   1023.599070      147.445933   3377.910241   \n\n           max_hr(t)         lf(t)        cvi(t)        sd1(t)     min_hr(t)  \\\ncount   10162.000000  10162.000000  10162.000000  10162.000000  10162.000000   \nunique           NaN           NaN           NaN           NaN           NaN   \ntop              NaN           NaN           NaN           NaN           NaN   \nfreq             NaN           NaN           NaN           NaN           NaN   \nmean       95.554944    154.574365      3.058102      4.668095     90.617402   \nstd        11.230745    296.929854      0.513247      3.225096     10.260242   \nmin        64.935065      1.186754      1.470822      0.801546     59.766537   \n25%        88.275862     29.891571      2.712418      2.512005     83.934426   \n50%        94.233129     72.679104      3.054160      3.752420     90.352941   \n75%       101.721854    166.617341      3.403178      5.851619     97.215190   \nmax       199.480519   7108.100125      4.978590     47.205406    142.222222   \n\n              sd2(t)     sampen(t)  median_nni(t)    mean_hr(t)   mean_nni(t)  \\\ncount   10162.000000  10162.000000   10162.000000  10162.000000  10162.000000   \nunique           NaN           NaN            NaN           NaN           NaN   \ntop              NaN           NaN            NaN           NaN           NaN   \nfreq             NaN           NaN            NaN           NaN           NaN   \nmean       23.324584      0.727263     657.784838     92.486451    657.041992   \nstd        17.481615      0.334321      73.168218     10.305441     72.822232   \nmin         2.059374      0.063899     371.093750     62.707951    382.412997   \n25%        11.678589      0.494644     609.375000     85.503687    607.910156   \n50%        18.414055      0.681358     652.343750     91.946834    652.604167   \n75%        29.684711      0.906523     703.125000     98.759764    702.008929   \nmax       152.819491      3.433987     960.000000    160.675088    957.454545   \n\n              csi(t)  Modified_csi(t)         label  \ncount   10162.000000     10162.000000  10162.000000  \nunique           NaN              NaN           NaN  \ntop              NaN              NaN           NaN  \nfreq             NaN              NaN           NaN  \nmean        5.431877       610.301583      0.129896  \nstd         2.995461       801.814200      0.336205  \nmin         0.771190        12.482404      0.000000  \n25%         3.467304       167.996028      0.000000  \n50%         4.746890       355.342190      0.000000  \n75%         6.610015       737.516720      0.000000  \nmax        36.710563     15007.698911      1.000000  \n\n[11 rows x 107 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Key</th>\n      <th>hf(t-6)</th>\n      <th>lf_hf_ratio(t-6)</th>\n      <th>vlf(t-6)</th>\n      <th>max_hr(t-6)</th>\n      <th>lf(t-6)</th>\n      <th>cvi(t-6)</th>\n      <th>sd1(t-6)</th>\n      <th>min_hr(t-6)</th>\n      <th>sd2(t-6)</th>\n      <th>sampen(t-6)</th>\n      <th>median_nni(t-6)</th>\n      <th>mean_hr(t-6)</th>\n      <th>mean_nni(t-6)</th>\n      <th>csi(t-6)</th>\n      <th>Modified_csi(t-6)</th>\n      <th>hf(t-5)</th>\n      <th>lf_hf_ratio(t-5)</th>\n      <th>vlf(t-5)</th>\n      <th>max_hr(t-5)</th>\n      <th>lf(t-5)</th>\n      <th>cvi(t-5)</th>\n      <th>sd1(t-5)</th>\n      <th>min_hr(t-5)</th>\n      <th>sd2(t-5)</th>\n      <th>sampen(t-5)</th>\n      <th>median_nni(t-5)</th>\n      <th>mean_hr(t-5)</th>\n      <th>mean_nni(t-5)</th>\n      <th>csi(t-5)</th>\n      <th>Modified_csi(t-5)</th>\n      <th>hf(t-4)</th>\n      <th>lf_hf_ratio(t-4)</th>\n      <th>vlf(t-4)</th>\n      <th>max_hr(t-4)</th>\n      <th>lf(t-4)</th>\n      <th>cvi(t-4)</th>\n      <th>sd1(t-4)</th>\n      <th>min_hr(t-4)</th>\n      <th>sd2(t-4)</th>\n      <th>sampen(t-4)</th>\n      <th>median_nni(t-4)</th>\n      <th>mean_hr(t-4)</th>\n      <th>mean_nni(t-4)</th>\n      <th>csi(t-4)</th>\n      <th>Modified_csi(t-4)</th>\n      <th>hf(t-3)</th>\n      <th>lf_hf_ratio(t-3)</th>\n      <th>vlf(t-3)</th>\n      <th>max_hr(t-3)</th>\n      <th>...</th>\n      <th>mean_hr(t-3)</th>\n      <th>mean_nni(t-3)</th>\n      <th>csi(t-3)</th>\n      <th>Modified_csi(t-3)</th>\n      <th>hf(t-2)</th>\n      <th>lf_hf_ratio(t-2)</th>\n      <th>vlf(t-2)</th>\n      <th>max_hr(t-2)</th>\n      <th>lf(t-2)</th>\n      <th>cvi(t-2)</th>\n      <th>sd1(t-2)</th>\n      <th>min_hr(t-2)</th>\n      <th>sd2(t-2)</th>\n      <th>sampen(t-2)</th>\n      <th>median_nni(t-2)</th>\n      <th>mean_hr(t-2)</th>\n      <th>mean_nni(t-2)</th>\n      <th>csi(t-2)</th>\n      <th>Modified_csi(t-2)</th>\n      <th>hf(t-1)</th>\n      <th>lf_hf_ratio(t-1)</th>\n      <th>vlf(t-1)</th>\n      <th>max_hr(t-1)</th>\n      <th>lf(t-1)</th>\n      <th>cvi(t-1)</th>\n      <th>sd1(t-1)</th>\n      <th>min_hr(t-1)</th>\n      <th>sd2(t-1)</th>\n      <th>sampen(t-1)</th>\n      <th>median_nni(t-1)</th>\n      <th>mean_hr(t-1)</th>\n      <th>mean_nni(t-1)</th>\n      <th>csi(t-1)</th>\n      <th>Modified_csi(t-1)</th>\n      <th>hf(t)</th>\n      <th>lf_hf_ratio(t)</th>\n      <th>vlf(t)</th>\n      <th>max_hr(t)</th>\n      <th>lf(t)</th>\n      <th>cvi(t)</th>\n      <th>sd1(t)</th>\n      <th>min_hr(t)</th>\n      <th>sd2(t)</th>\n      <th>sampen(t)</th>\n      <th>median_nni(t)</th>\n      <th>mean_hr(t)</th>\n      <th>mean_nni(t)</th>\n      <th>csi(t)</th>\n      <th>Modified_csi(t)</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>10162</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>...</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n      <td>10162.000000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>202</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>dev_01_tcp_ar_9578_s021_2013_08_19_t001</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>334</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>NaN</td>\n      <td>13.193353</td>\n      <td>16.161888</td>\n      <td>121.553360</td>\n      <td>95.616836</td>\n      <td>154.608713</td>\n      <td>3.058257</td>\n      <td>4.660172</td>\n      <td>90.652119</td>\n      <td>23.405456</td>\n      <td>0.725931</td>\n      <td>657.309157</td>\n      <td>92.525921</td>\n      <td>656.565873</td>\n      <td>5.458394</td>\n      <td>616.210998</td>\n      <td>13.202052</td>\n      <td>16.150197</td>\n      <td>121.413257</td>\n      <td>95.591724</td>\n      <td>154.694490</td>\n      <td>3.057858</td>\n      <td>4.659211</td>\n      <td>90.642698</td>\n      <td>23.379304</td>\n      <td>0.726619</td>\n      <td>657.339153</td>\n      <td>92.518562</td>\n      <td>656.610956</td>\n      <td>5.453695</td>\n      <td>614.718494</td>\n      <td>13.193538</td>\n      <td>16.133862</td>\n      <td>121.245740</td>\n      <td>95.585314</td>\n      <td>154.458557</td>\n      <td>3.057768</td>\n      <td>4.659560</td>\n      <td>90.637588</td>\n      <td>23.358871</td>\n      <td>0.727097</td>\n      <td>657.452077</td>\n      <td>92.513347</td>\n      <td>656.723477</td>\n      <td>5.448779</td>\n      <td>613.607499</td>\n      <td>13.208245</td>\n      <td>16.126224</td>\n      <td>121.238918</td>\n      <td>95.574190</td>\n      <td>...</td>\n      <td>92.503103</td>\n      <td>656.800012</td>\n      <td>5.444579</td>\n      <td>612.856547</td>\n      <td>13.211123</td>\n      <td>16.115817</td>\n      <td>121.192930</td>\n      <td>95.564185</td>\n      <td>154.461406</td>\n      <td>3.057695</td>\n      <td>4.661256</td>\n      <td>90.616864</td>\n      <td>23.333946</td>\n      <td>0.727117</td>\n      <td>657.630550</td>\n      <td>92.492177</td>\n      <td>656.889975</td>\n      <td>5.440220</td>\n      <td>612.221739</td>\n      <td>13.240601</td>\n      <td>16.117698</td>\n      <td>121.244891</td>\n      <td>95.559541</td>\n      <td>154.621150</td>\n      <td>3.057848</td>\n      <td>4.664603</td>\n      <td>90.610947</td>\n      <td>23.326801</td>\n      <td>0.727365</td>\n      <td>657.791762</td>\n      <td>92.484198</td>\n      <td>657.042332</td>\n      <td>5.434736</td>\n      <td>611.028277</td>\n      <td>13.255576</td>\n      <td>16.117865</td>\n      <td>121.038902</td>\n      <td>95.554944</td>\n      <td>154.574365</td>\n      <td>3.058102</td>\n      <td>4.668095</td>\n      <td>90.617402</td>\n      <td>23.324584</td>\n      <td>0.727263</td>\n      <td>657.784838</td>\n      <td>92.486451</td>\n      <td>657.041992</td>\n      <td>5.431877</td>\n      <td>610.301583</td>\n      <td>0.129896</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>NaN</td>\n      <td>29.587876</td>\n      <td>14.114258</td>\n      <td>227.118045</td>\n      <td>11.260092</td>\n      <td>295.489349</td>\n      <td>0.506574</td>\n      <td>3.199092</td>\n      <td>10.244034</td>\n      <td>17.250134</td>\n      <td>0.325818</td>\n      <td>73.456321</td>\n      <td>10.276343</td>\n      <td>73.131143</td>\n      <td>2.926033</td>\n      <td>786.683205</td>\n      <td>29.613253</td>\n      <td>14.119956</td>\n      <td>227.141263</td>\n      <td>11.211105</td>\n      <td>295.687997</td>\n      <td>0.507516</td>\n      <td>3.203542</td>\n      <td>10.225034</td>\n      <td>17.276516</td>\n      <td>0.327494</td>\n      <td>73.427224</td>\n      <td>10.273398</td>\n      <td>73.110828</td>\n      <td>2.934942</td>\n      <td>788.101950</td>\n      <td>29.599020</td>\n      <td>14.129624</td>\n      <td>227.193073</td>\n      <td>11.217543</td>\n      <td>295.479418</td>\n      <td>0.508284</td>\n      <td>3.204942</td>\n      <td>10.230340</td>\n      <td>17.309044</td>\n      <td>0.329171</td>\n      <td>73.174669</td>\n      <td>10.276739</td>\n      <td>72.854548</td>\n      <td>2.947190</td>\n      <td>790.447446</td>\n      <td>29.638782</td>\n      <td>14.155953</td>\n      <td>227.468660</td>\n      <td>11.218700</td>\n      <td>...</td>\n      <td>10.277674</td>\n      <td>72.873766</td>\n      <td>2.962006</td>\n      <td>794.069903</td>\n      <td>29.646893</td>\n      <td>14.191359</td>\n      <td>227.788720</td>\n      <td>11.215230</td>\n      <td>295.935120</td>\n      <td>0.510509</td>\n      <td>3.210132</td>\n      <td>10.237693</td>\n      <td>17.392679</td>\n      <td>0.332323</td>\n      <td>73.279488</td>\n      <td>10.282690</td>\n      <td>72.938276</td>\n      <td>2.976300</td>\n      <td>797.852625</td>\n      <td>29.730434</td>\n      <td>14.261396</td>\n      <td>228.172750</td>\n      <td>11.224031</td>\n      <td>296.474584</td>\n      <td>0.511871</td>\n      <td>3.217505</td>\n      <td>10.250864</td>\n      <td>17.440551</td>\n      <td>0.333284</td>\n      <td>73.095392</td>\n      <td>10.292418</td>\n      <td>72.760950</td>\n      <td>2.984035</td>\n      <td>799.596247</td>\n      <td>29.773111</td>\n      <td>14.353969</td>\n      <td>228.275105</td>\n      <td>11.230745</td>\n      <td>296.929854</td>\n      <td>0.513247</td>\n      <td>3.225096</td>\n      <td>10.260242</td>\n      <td>17.481615</td>\n      <td>0.334321</td>\n      <td>73.168218</td>\n      <td>10.305441</td>\n      <td>72.822232</td>\n      <td>2.995461</td>\n      <td>801.814200</td>\n      <td>0.336205</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>NaN</td>\n      <td>0.402726</td>\n      <td>0.714182</td>\n      <td>0.196191</td>\n      <td>67.873303</td>\n      <td>1.186754</td>\n      <td>1.470822</td>\n      <td>0.801546</td>\n      <td>59.766537</td>\n      <td>2.059374</td>\n      <td>0.063899</td>\n      <td>0.000000</td>\n      <td>65.806031</td>\n      <td>0.000000</td>\n      <td>0.771190</td>\n      <td>12.482404</td>\n      <td>0.402726</td>\n      <td>0.714182</td>\n      <td>0.196191</td>\n      <td>67.873303</td>\n      <td>1.186754</td>\n      <td>1.470822</td>\n      <td>0.801546</td>\n      <td>59.766537</td>\n      <td>2.059374</td>\n      <td>0.063899</td>\n      <td>0.000000</td>\n      <td>65.806031</td>\n      <td>0.000000</td>\n      <td>0.771190</td>\n      <td>12.482404</td>\n      <td>0.402726</td>\n      <td>0.714182</td>\n      <td>0.196191</td>\n      <td>67.873303</td>\n      <td>1.186754</td>\n      <td>1.470822</td>\n      <td>0.801546</td>\n      <td>59.766537</td>\n      <td>2.059374</td>\n      <td>0.063899</td>\n      <td>0.000000</td>\n      <td>65.806031</td>\n      <td>0.000000</td>\n      <td>0.771190</td>\n      <td>12.482404</td>\n      <td>0.402726</td>\n      <td>0.714182</td>\n      <td>0.196191</td>\n      <td>65.502183</td>\n      <td>...</td>\n      <td>63.546436</td>\n      <td>0.000000</td>\n      <td>0.771190</td>\n      <td>12.482404</td>\n      <td>0.402726</td>\n      <td>0.714182</td>\n      <td>0.196191</td>\n      <td>65.502183</td>\n      <td>1.186754</td>\n      <td>1.470822</td>\n      <td>0.801546</td>\n      <td>59.766537</td>\n      <td>2.059374</td>\n      <td>0.063899</td>\n      <td>0.000000</td>\n      <td>63.546436</td>\n      <td>0.000000</td>\n      <td>0.771190</td>\n      <td>12.482404</td>\n      <td>0.402726</td>\n      <td>0.714182</td>\n      <td>0.196191</td>\n      <td>64.935065</td>\n      <td>1.186754</td>\n      <td>1.470822</td>\n      <td>0.801546</td>\n      <td>59.766537</td>\n      <td>2.059374</td>\n      <td>0.063899</td>\n      <td>371.093750</td>\n      <td>62.707951</td>\n      <td>382.412997</td>\n      <td>0.771190</td>\n      <td>12.482404</td>\n      <td>0.402726</td>\n      <td>0.714182</td>\n      <td>0.196191</td>\n      <td>64.935065</td>\n      <td>1.186754</td>\n      <td>1.470822</td>\n      <td>0.801546</td>\n      <td>59.766537</td>\n      <td>2.059374</td>\n      <td>0.063899</td>\n      <td>371.093750</td>\n      <td>62.707951</td>\n      <td>382.412997</td>\n      <td>0.771190</td>\n      <td>12.482404</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>NaN</td>\n      <td>2.647099</td>\n      <td>6.403305</td>\n      <td>19.650680</td>\n      <td>88.275862</td>\n      <td>31.134530</td>\n      <td>2.724546</td>\n      <td>2.542668</td>\n      <td>83.934426</td>\n      <td>11.881367</td>\n      <td>0.502114</td>\n      <td>609.375000</td>\n      <td>85.558472</td>\n      <td>607.177734</td>\n      <td>3.532118</td>\n      <td>172.551041</td>\n      <td>2.646750</td>\n      <td>6.403305</td>\n      <td>19.471676</td>\n      <td>88.275862</td>\n      <td>30.826074</td>\n      <td>2.722456</td>\n      <td>2.537167</td>\n      <td>83.934426</td>\n      <td>11.829085</td>\n      <td>0.501123</td>\n      <td>609.375000</td>\n      <td>85.552784</td>\n      <td>607.421875</td>\n      <td>3.525008</td>\n      <td>171.960201</td>\n      <td>2.634705</td>\n      <td>6.384462</td>\n      <td>19.264986</td>\n      <td>88.275862</td>\n      <td>30.655258</td>\n      <td>2.721479</td>\n      <td>2.532909</td>\n      <td>83.934426</td>\n      <td>11.807143</td>\n      <td>0.499905</td>\n      <td>609.375000</td>\n      <td>85.539833</td>\n      <td>607.421875</td>\n      <td>3.512167</td>\n      <td>171.114292</td>\n      <td>2.629791</td>\n      <td>6.369572</td>\n      <td>19.126927</td>\n      <td>88.275862</td>\n      <td>...</td>\n      <td>85.526079</td>\n      <td>607.536765</td>\n      <td>3.498556</td>\n      <td>170.088308</td>\n      <td>2.620090</td>\n      <td>6.346912</td>\n      <td>19.033227</td>\n      <td>88.275862</td>\n      <td>30.372360</td>\n      <td>2.716477</td>\n      <td>2.525518</td>\n      <td>83.934426</td>\n      <td>11.742268</td>\n      <td>0.497914</td>\n      <td>609.375000</td>\n      <td>85.512200</td>\n      <td>607.666016</td>\n      <td>3.489128</td>\n      <td>169.442786</td>\n      <td>2.615222</td>\n      <td>6.345730</td>\n      <td>18.891034</td>\n      <td>88.275862</td>\n      <td>30.188127</td>\n      <td>2.713910</td>\n      <td>2.518875</td>\n      <td>83.934426</td>\n      <td>11.710161</td>\n      <td>0.496647</td>\n      <td>609.375000</td>\n      <td>85.504068</td>\n      <td>607.802447</td>\n      <td>3.479076</td>\n      <td>168.624719</td>\n      <td>2.607297</td>\n      <td>6.315848</td>\n      <td>18.670768</td>\n      <td>88.275862</td>\n      <td>29.891571</td>\n      <td>2.712418</td>\n      <td>2.512005</td>\n      <td>83.934426</td>\n      <td>11.678589</td>\n      <td>0.494644</td>\n      <td>609.375000</td>\n      <td>85.503687</td>\n      <td>607.910156</td>\n      <td>3.467304</td>\n      <td>167.996028</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>NaN</td>\n      <td>5.617209</td>\n      <td>12.930494</td>\n      <td>54.882191</td>\n      <td>94.233129</td>\n      <td>73.608850</td>\n      <td>3.051493</td>\n      <td>3.743001</td>\n      <td>90.352941</td>\n      <td>18.752836</td>\n      <td>0.682382</td>\n      <td>652.343750</td>\n      <td>91.957896</td>\n      <td>652.604167</td>\n      <td>4.820640</td>\n      <td>377.828466</td>\n      <td>5.610511</td>\n      <td>12.867016</td>\n      <td>54.727040</td>\n      <td>94.233129</td>\n      <td>73.426403</td>\n      <td>3.050966</td>\n      <td>3.737422</td>\n      <td>90.352941</td>\n      <td>18.705508</td>\n      <td>0.682492</td>\n      <td>652.343750</td>\n      <td>91.953200</td>\n      <td>652.604167</td>\n      <td>4.807779</td>\n      <td>376.104969</td>\n      <td>5.608777</td>\n      <td>12.820092</td>\n      <td>54.672552</td>\n      <td>94.233129</td>\n      <td>73.100181</td>\n      <td>3.050709</td>\n      <td>3.738209</td>\n      <td>90.352941</td>\n      <td>18.605535</td>\n      <td>0.682639</td>\n      <td>652.343750</td>\n      <td>91.953200</td>\n      <td>652.604167</td>\n      <td>4.791672</td>\n      <td>370.879433</td>\n      <td>5.604066</td>\n      <td>12.781229</td>\n      <td>54.248020</td>\n      <td>94.233129</td>\n      <td>...</td>\n      <td>91.948182</td>\n      <td>652.604167</td>\n      <td>4.782166</td>\n      <td>366.736460</td>\n      <td>5.574274</td>\n      <td>12.761474</td>\n      <td>53.561327</td>\n      <td>94.233129</td>\n      <td>72.859567</td>\n      <td>3.052602</td>\n      <td>3.744269</td>\n      <td>90.352941</td>\n      <td>18.493598</td>\n      <td>0.681751</td>\n      <td>652.343750</td>\n      <td>91.953972</td>\n      <td>652.604167</td>\n      <td>4.763840</td>\n      <td>363.567283</td>\n      <td>5.552642</td>\n      <td>12.717142</td>\n      <td>53.085749</td>\n      <td>94.233129</td>\n      <td>72.859567</td>\n      <td>3.053419</td>\n      <td>3.746811</td>\n      <td>90.352941</td>\n      <td>18.452765</td>\n      <td>0.681620</td>\n      <td>652.343750</td>\n      <td>91.944863</td>\n      <td>652.604167</td>\n      <td>4.753710</td>\n      <td>359.297258</td>\n      <td>5.497420</td>\n      <td>12.665044</td>\n      <td>52.512485</td>\n      <td>94.233129</td>\n      <td>72.679104</td>\n      <td>3.054160</td>\n      <td>3.752420</td>\n      <td>90.352941</td>\n      <td>18.414055</td>\n      <td>0.681358</td>\n      <td>652.343750</td>\n      <td>91.946834</td>\n      <td>652.604167</td>\n      <td>4.746890</td>\n      <td>355.342190</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>NaN</td>\n      <td>12.013642</td>\n      <td>20.705437</td>\n      <td>129.374120</td>\n      <td>101.721854</td>\n      <td>163.894554</td>\n      <td>3.394944</td>\n      <td>5.823403</td>\n      <td>97.215190</td>\n      <td>29.604736</td>\n      <td>0.897739</td>\n      <td>703.125000</td>\n      <td>98.867548</td>\n      <td>701.450893</td>\n      <td>6.662837</td>\n      <td>760.727809</td>\n      <td>12.013642</td>\n      <td>20.705437</td>\n      <td>129.374120</td>\n      <td>101.721854</td>\n      <td>164.134068</td>\n      <td>3.395798</td>\n      <td>5.826257</td>\n      <td>97.215190</td>\n      <td>29.592565</td>\n      <td>0.900663</td>\n      <td>703.125000</td>\n      <td>98.819902</td>\n      <td>701.676339</td>\n      <td>6.656457</td>\n      <td>756.185376</td>\n      <td>12.013642</td>\n      <td>20.703865</td>\n      <td>128.166316</td>\n      <td>101.721854</td>\n      <td>164.435367</td>\n      <td>3.396226</td>\n      <td>5.835041</td>\n      <td>97.215190</td>\n      <td>29.592565</td>\n      <td>0.902868</td>\n      <td>703.125000</td>\n      <td>98.819902</td>\n      <td>701.729911</td>\n      <td>6.648563</td>\n      <td>752.950486</td>\n      <td>12.041350</td>\n      <td>20.683849</td>\n      <td>127.352379</td>\n      <td>101.721854</td>\n      <td>...</td>\n      <td>98.817499</td>\n      <td>701.729911</td>\n      <td>6.641931</td>\n      <td>748.453376</td>\n      <td>12.096541</td>\n      <td>20.670866</td>\n      <td>126.839991</td>\n      <td>101.721854</td>\n      <td>165.969172</td>\n      <td>3.399983</td>\n      <td>5.839973</td>\n      <td>97.215190</td>\n      <td>29.656429</td>\n      <td>0.904662</td>\n      <td>703.125000</td>\n      <td>98.779926</td>\n      <td>701.822917</td>\n      <td>6.628589</td>\n      <td>744.020490</td>\n      <td>12.147710</td>\n      <td>20.670103</td>\n      <td>126.625348</td>\n      <td>101.721854</td>\n      <td>166.612037</td>\n      <td>3.401522</td>\n      <td>5.844929</td>\n      <td>97.215190</td>\n      <td>29.656429</td>\n      <td>0.905322</td>\n      <td>703.125000</td>\n      <td>98.765365</td>\n      <td>702.006696</td>\n      <td>6.617435</td>\n      <td>740.018576</td>\n      <td>12.190304</td>\n      <td>20.670103</td>\n      <td>126.403313</td>\n      <td>101.721854</td>\n      <td>166.617341</td>\n      <td>3.403178</td>\n      <td>5.851619</td>\n      <td>97.215190</td>\n      <td>29.684711</td>\n      <td>0.906523</td>\n      <td>703.125000</td>\n      <td>98.759764</td>\n      <td>702.008929</td>\n      <td>6.610015</td>\n      <td>737.516720</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>NaN</td>\n      <td>1023.599070</td>\n      <td>147.445933</td>\n      <td>3377.910241</td>\n      <td>199.480519</td>\n      <td>7108.100125</td>\n      <td>4.978590</td>\n      <td>47.205406</td>\n      <td>142.883721</td>\n      <td>152.819491</td>\n      <td>3.433987</td>\n      <td>928.000000</td>\n      <td>160.675088</td>\n      <td>912.642045</td>\n      <td>36.710563</td>\n      <td>15007.698911</td>\n      <td>1023.599070</td>\n      <td>147.445933</td>\n      <td>3377.910241</td>\n      <td>199.480519</td>\n      <td>7108.100125</td>\n      <td>4.978590</td>\n      <td>47.205406</td>\n      <td>142.222222</td>\n      <td>152.819491</td>\n      <td>3.433987</td>\n      <td>928.000000</td>\n      <td>160.675088</td>\n      <td>912.642045</td>\n      <td>36.710563</td>\n      <td>15007.698911</td>\n      <td>1023.599070</td>\n      <td>147.445933</td>\n      <td>3377.910241</td>\n      <td>199.480519</td>\n      <td>7108.100125</td>\n      <td>4.978590</td>\n      <td>47.205406</td>\n      <td>142.222222</td>\n      <td>152.819491</td>\n      <td>3.433987</td>\n      <td>928.000000</td>\n      <td>160.675088</td>\n      <td>912.642045</td>\n      <td>36.710563</td>\n      <td>15007.698911</td>\n      <td>1023.599070</td>\n      <td>147.445933</td>\n      <td>3377.910241</td>\n      <td>199.480519</td>\n      <td>...</td>\n      <td>160.675088</td>\n      <td>944.363636</td>\n      <td>36.710563</td>\n      <td>15007.698911</td>\n      <td>1023.599070</td>\n      <td>147.445933</td>\n      <td>3377.910241</td>\n      <td>199.480519</td>\n      <td>7108.100125</td>\n      <td>4.978590</td>\n      <td>47.205406</td>\n      <td>142.222222</td>\n      <td>152.819491</td>\n      <td>3.433987</td>\n      <td>944.000000</td>\n      <td>160.675088</td>\n      <td>944.363636</td>\n      <td>36.710563</td>\n      <td>15007.698911</td>\n      <td>1023.599070</td>\n      <td>147.445933</td>\n      <td>3377.910241</td>\n      <td>199.480519</td>\n      <td>7108.100125</td>\n      <td>4.978590</td>\n      <td>47.205406</td>\n      <td>142.222222</td>\n      <td>152.819491</td>\n      <td>3.433987</td>\n      <td>960.000000</td>\n      <td>160.675088</td>\n      <td>957.454545</td>\n      <td>36.710563</td>\n      <td>15007.698911</td>\n      <td>1023.599070</td>\n      <td>147.445933</td>\n      <td>3377.910241</td>\n      <td>199.480519</td>\n      <td>7108.100125</td>\n      <td>4.978590</td>\n      <td>47.205406</td>\n      <td>142.222222</td>\n      <td>152.819491</td>\n      <td>3.433987</td>\n      <td>960.000000</td>\n      <td>160.675088</td>\n      <td>957.454545</td>\n      <td>36.710563</td>\n      <td>15007.698911</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>11 rows × 107 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwLsrd4rQQOd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5006cccb-bf96-4362-86dc-e63fb6e923c0",
        "tags": [
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend"
        ]
      },
      "source": [
        "df_f = dict_df['60'][dict_df['60'].Key.str.contains('9578')]\n",
        "X = df_f.iloc[:, 1:-1].values\n",
        "Y = df_f.iloc[:, -1:].values\n",
        "\n",
        "X_train_fit, X_test_fit, Y_train, Y_test = train_test_split(X, Y, random_state=42, stratify=Y, train_size=0.8)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_fit = scaler.fit_transform(X_train_fit)\n",
        "X_test_fit = scaler.transform(X_test_fit)\n",
        "\n",
        "nb_period = 6\n",
        "\n",
        "#reshape specifique en 3D pour le format attendu pour le LSTM\n",
        "param2 = int(X_train_fit.shape[1]/(nb_period+1))\n",
        "X_train_resh = X_train_fit.reshape((X_train_fit.shape[0],nb_period+1,param2))\n",
        "X_test_resh = X_test_fit.reshape((X_test_fit.shape[0], nb_period+1, param2))\n",
        "#print(X_train_resh.shape, Y_train.shape, X_test_resh.shape, Y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.LSTM(600, input_shape=(X_train_resh.shape[1], X_train_resh.shape[2])))\n",
        "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Créons un learning rate schedule pour décroitre le learning rate à mesure que nous entrainons le modèle \n",
        "initial_learning_rate = 0.001\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=1090,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "# Utilisation d'un compileur simple avec un optimiseur Adam pour le calcul de nos gradients \n",
        "optimizer= tf.keras.optimizers.Adam(\n",
        "    learning_rate = lr_schedule\n",
        ")\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.BinaryCrossentropy(),\n",
        "                      tf.keras.metrics.Precision(),\n",
        "                      tf.keras.metrics.Recall()])\n",
        "\n",
        "history = model.fit(X_train_resh, Y_train, epochs=100, batch_size=36, validation_data=(X_test_resh, Y_test), verbose=1, shuffle=False)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "precision: 0.8272 - val_recall: 0.7614\nEpoch 21/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0567 - binary_crossentropy: 0.0567 - precision: 0.9452 - recall: 0.8987 - val_loss: 0.1380 - val_binary_crossentropy: 0.1380 - val_precision: 0.8038 - val_recall: 0.8068\nEpoch 22/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0519 - binary_crossentropy: 0.0519 - precision: 0.9464 - recall: 0.9025 - val_loss: 0.1481 - val_binary_crossentropy: 0.1481 - val_precision: 0.7978 - val_recall: 0.8068\nEpoch 23/100\n226/226 [==============================] - 4s 20ms/step - loss: 0.0481 - binary_crossentropy: 0.0481 - precision: 0.9513 - recall: 0.9072 - val_loss: 0.1284 - val_binary_crossentropy: 0.1284 - val_precision: 0.8477 - val_recall: 0.8220\nEpoch 24/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0397 - binary_crossentropy: 0.0397 - precision: 0.9674 - recall: 0.9271 - val_loss: 0.1362 - val_binary_crossentropy: 0.1362 - val_precision: 0.8554 - val_recall: 0.8068\nEpoch 25/100\n226/226 [==============================] - 4s 20ms/step - loss: 0.0527 - binary_crossentropy: 0.0527 - precision: 0.9472 - recall: 0.9006 - val_loss: 0.1315 - val_binary_crossentropy: 0.1315 - val_precision: 0.8081 - val_recall: 0.8295\nEpoch 26/100\n226/226 [==============================] - 5s 22ms/step - loss: 0.0379 - binary_crossentropy: 0.0379 - precision: 0.9627 - recall: 0.9299 - val_loss: 0.1357 - val_binary_crossentropy: 0.1357 - val_precision: 0.8555 - val_recall: 0.8295\nEpoch 27/100\n226/226 [==============================] - 4s 20ms/step - loss: 0.0378 - binary_crossentropy: 0.0378 - precision: 0.9601 - recall: 0.9337 - val_loss: 0.1469 - val_binary_crossentropy: 0.1469 - val_precision: 0.8244 - val_recall: 0.8182\nEpoch 28/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0340 - binary_crossentropy: 0.0340 - precision: 0.9687 - recall: 0.9366 - val_loss: 0.1508 - val_binary_crossentropy: 0.1508 - val_precision: 0.8431 - val_recall: 0.8144\nEpoch 29/100\n226/226 [==============================] - 4s 20ms/step - loss: 0.0434 - binary_crossentropy: 0.0434 - precision: 0.9616 - recall: 0.9252 - val_loss: 0.1139 - val_binary_crossentropy: 0.1139 - val_precision: 0.8821 - val_recall: 0.8220\nEpoch 30/100\n226/226 [==============================] - 5s 22ms/step - loss: 0.0382 - binary_crossentropy: 0.0382 - precision: 0.9590 - recall: 0.9299 - val_loss: 0.1501 - val_binary_crossentropy: 0.1501 - val_precision: 0.8189 - val_recall: 0.7879\nEpoch 31/100\n226/226 [==============================] - 4s 20ms/step - loss: 0.0278 - binary_crossentropy: 0.0278 - precision: 0.9775 - recall: 0.9470 - val_loss: 0.1328 - val_binary_crossentropy: 0.1328 - val_precision: 0.8617 - val_recall: 0.8258\nEpoch 32/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0305 - binary_crossentropy: 0.0305 - precision: 0.9642 - recall: 0.9441 - val_loss: 0.1227 - val_binary_crossentropy: 0.1227 - val_precision: 0.8629 - val_recall: 0.8106\nEpoch 33/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0270 - binary_crossentropy: 0.0270 - precision: 0.9709 - recall: 0.9489 - val_loss: 0.1317 - val_binary_crossentropy: 0.1317 - val_precision: 0.8894 - val_recall: 0.7917\nEpoch 34/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0278 - binary_crossentropy: 0.0278 - precision: 0.9746 - recall: 0.9460 - val_loss: 0.1288 - val_binary_crossentropy: 0.1288 - val_precision: 0.8784 - val_recall: 0.8485\nEpoch 35/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0193 - binary_crossentropy: 0.0193 - precision: 0.9817 - recall: 0.9650 - val_loss: 0.1210 - val_binary_crossentropy: 0.1210 - val_precision: 0.8735 - val_recall: 0.8371\nEpoch 36/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0252 - binary_crossentropy: 0.0252 - precision: 0.9749 - recall: 0.9574 - val_loss: 0.1306 - val_binary_crossentropy: 0.1306 - val_precision: 0.8271 - val_recall: 0.8333\nEpoch 37/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0299 - binary_crossentropy: 0.0299 - precision: 0.9699 - recall: 0.9451 - val_loss: 0.1393 - val_binary_crossentropy: 0.1393 - val_precision: 0.8244 - val_recall: 0.8712\nEpoch 38/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0277 - binary_crossentropy: 0.0277 - precision: 0.9683 - recall: 0.9555 - val_loss: 0.1319 - val_binary_crossentropy: 0.1319 - val_precision: 0.8631 - val_recall: 0.8598\nEpoch 39/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0262 - binary_crossentropy: 0.0262 - precision: 0.9740 - recall: 0.9564 - val_loss: 0.1175 - val_binary_crossentropy: 0.1175 - val_precision: 0.8803 - val_recall: 0.8636\nEpoch 40/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0186 - binary_crossentropy: 0.0186 - precision: 0.9780 - recall: 0.9688 - val_loss: 0.1241 - val_binary_crossentropy: 0.1241 - val_precision: 0.8840 - val_recall: 0.8371\nEpoch 41/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0170 - binary_crossentropy: 0.0170 - precision: 0.9779 - recall: 0.9650 - val_loss: 0.1000 - val_binary_crossentropy: 0.1000 - val_precision: 0.8842 - val_recall: 0.8674\nEpoch 42/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0217 - binary_crossentropy: 0.0217 - precision: 0.9733 - recall: 0.9650 - val_loss: 0.1082 - val_binary_crossentropy: 0.1082 - val_precision: 0.8462 - val_recall: 0.8750\nEpoch 43/100\n226/226 [==============================] - 4s 20ms/step - loss: 0.0169 - binary_crossentropy: 0.0169 - precision: 0.9836 - recall: 0.9659 - val_loss: 0.1398 - val_binary_crossentropy: 0.1398 - val_precision: 0.8390 - val_recall: 0.8485\nEpoch 44/100\n226/226 [==============================] - 4s 19ms/step - loss: 0.0119 - binary_crossentropy: 0.0119 - precision: 0.9866 - recall: 0.9754 - val_loss: 0.1153 - val_binary_crossentropy: 0.1153 - val_precision: 0.8949 - val_recall: 0.8712\nEpoch 45/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0102 - binary_crossentropy: 0.0102 - precision: 0.9857 - recall: 0.9792 - val_loss: 0.1219 - val_binary_crossentropy: 0.1219 - val_precision: 0.9073 - val_recall: 0.8523\nEpoch 46/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0110 - binary_crossentropy: 0.0110 - precision: 0.9885 - recall: 0.9801 - val_loss: 0.1194 - val_binary_crossentropy: 0.1194 - val_precision: 0.8937 - val_recall: 0.8598\nEpoch 47/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0237 - binary_crossentropy: 0.0237 - precision: 0.9760 - recall: 0.9612 - val_loss: 0.1484 - val_binary_crossentropy: 0.1484 - val_precision: 0.8423 - val_recall: 0.8295\nEpoch 48/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0195 - binary_crossentropy: 0.0195 - precision: 0.9780 - recall: 0.9697 - val_loss: 0.1399 - val_binary_crossentropy: 0.1399 - val_precision: 0.8672 - val_recall: 0.8409\nEpoch 49/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0162 - binary_crossentropy: 0.0162 - precision: 0.9772 - recall: 0.9754 - val_loss: 0.1487 - val_binary_crossentropy: 0.1487 - val_precision: 0.8510 - val_recall: 0.8220\nEpoch 50/100\n226/226 [==============================] - 4s 20ms/step - loss: 0.0165 - binary_crossentropy: 0.0165 - precision: 0.9772 - recall: 0.9725 - val_loss: 0.1224 - val_binary_crossentropy: 0.1224 - val_precision: 0.8889 - val_recall: 0.8788\nEpoch 51/100\n226/226 [==============================] - 4s 20ms/step - loss: 0.0088 - binary_crossentropy: 0.0088 - precision: 0.9915 - recall: 0.9886 - val_loss: 0.1294 - val_binary_crossentropy: 0.1294 - val_precision: 0.9036 - val_recall: 0.8523\nEpoch 52/100\n226/226 [==============================] - 4s 19ms/step - loss: 0.0086 - binary_crossentropy: 0.0086 - precision: 0.9886 - recall: 0.9858 - val_loss: 0.1364 - val_binary_crossentropy: 0.1364 - val_precision: 0.8745 - val_recall: 0.8712\nEpoch 53/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0080 - binary_crossentropy: 0.0080 - precision: 0.9914 - recall: 0.9858 - val_loss: 0.1163 - val_binary_crossentropy: 0.1163 - val_precision: 0.9098 - val_recall: 0.8788\nEpoch 54/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0075 - binary_crossentropy: 0.0075 - precision: 0.9895 - recall: 0.9848 - val_loss: 0.1341 - val_binary_crossentropy: 0.1341 - val_precision: 0.8842 - val_recall: 0.8674\nEpoch 55/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0183 - binary_crossentropy: 0.0183 - precision: 0.9875 - recall: 0.9725 - val_loss: 0.1983 - val_binary_crossentropy: 0.1983 - val_precision: 0.8106 - val_recall: 0.8106\nEpoch 56/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0438 - binary_crossentropy: 0.0438 - precision: 0.9538 - recall: 0.9394 - val_loss: 0.1373 - val_binary_crossentropy: 0.1373 - val_precision: 0.8721 - val_recall: 0.8523\nEpoch 57/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0228 - binary_crossentropy: 0.0228 - precision: 0.9807 - recall: 0.9621 - val_loss: 0.1343 - val_binary_crossentropy: 0.1343 - val_precision: 0.8945 - val_recall: 0.8674\nEpoch 58/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0088 - binary_crossentropy: 0.0088 - precision: 0.9924 - recall: 0.9877 - val_loss: 0.1315 - val_binary_crossentropy: 0.1315 - val_precision: 0.8837 - val_recall: 0.8636\nEpoch 59/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0071 - binary_crossentropy: 0.0071 - precision: 0.9933 - recall: 0.9886 - val_loss: 0.1506 - val_binary_crossentropy: 0.1506 - val_precision: 0.8524 - val_recall: 0.8750\nEpoch 60/100\n226/226 [==============================] - 5s 22ms/step - loss: 0.0064 - binary_crossentropy: 0.0064 - precision: 0.9915 - recall: 0.9934 - val_loss: 0.1351 - val_binary_crossentropy: 0.1351 - val_precision: 0.8846 - val_recall: 0.8712\nEpoch 61/100\n226/226 [==============================] - 4s 20ms/step - loss: 0.0046 - binary_crossentropy: 0.0046 - precision: 0.9934 - recall: 0.9934 - val_loss: 0.1377 - val_binary_crossentropy: 0.1377 - val_precision: 0.8864 - val_recall: 0.8864\nEpoch 62/100\n226/226 [==============================] - 4s 19ms/step - loss: 0.0120 - binary_crossentropy: 0.0120 - precision: 0.9848 - recall: 0.9839 - val_loss: 0.1526 - val_binary_crossentropy: 0.1526 - val_precision: 0.8352 - val_recall: 0.8636\nEpoch 63/100\n226/226 [==============================] - 4s 20ms/step - loss: 0.0324 - binary_crossentropy: 0.0324 - precision: 0.9682 - recall: 0.9508 - val_loss: 0.1497 - val_binary_crossentropy: 0.1497 - val_precision: 0.8725 - val_recall: 0.8295\nEpoch 64/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0123 - binary_crossentropy: 0.0123 - precision: 0.9876 - recall: 0.9830 - val_loss: 0.1149 - val_binary_crossentropy: 0.1149 - val_precision: 0.8859 - val_recall: 0.8826\nEpoch 65/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0056 - binary_crossentropy: 0.0056 - precision: 0.9943 - recall: 0.9915 - val_loss: 0.1144 - val_binary_crossentropy: 0.1144 - val_precision: 0.8949 - val_recall: 0.8712\nEpoch 66/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0046 - binary_crossentropy: 0.0046 - precision: 0.9962 - recall: 0.9905 - val_loss: 0.1166 - val_binary_crossentropy: 0.1166 - val_precision: 0.8953 - val_recall: 0.8750\nEpoch 67/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0033 - binary_crossentropy: 0.0033 - precision: 0.9962 - recall: 0.9981 - val_loss: 0.1142 - val_binary_crossentropy: 0.1142 - val_precision: 0.9035 - val_recall: 0.8864\nEpoch 68/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0100 - binary_crossentropy: 0.0100 - precision: 0.9877 - recall: 0.9877 - val_loss: 0.1307 - val_binary_crossentropy: 0.1307 - val_precision: 0.8755 - val_recall: 0.8788\nEpoch 69/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0113 - binary_crossentropy: 0.0113 - precision: 0.9867 - recall: 0.9839 - val_loss: 0.1017 - val_binary_crossentropy: 0.1017 - val_precision: 0.9167 - val_recall: 0.8750\nEpoch 70/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0101 - binary_crossentropy: 0.0101 - precision: 0.9886 - recall: 0.9886 - val_loss: 0.1322 - val_binary_crossentropy: 0.1322 - val_precision: 0.8625 - val_recall: 0.8788\nEpoch 71/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0089 - binary_crossentropy: 0.0089 - precision: 0.9943 - recall: 0.9886 - val_loss: 0.1170 - val_binary_crossentropy: 0.1170 - val_precision: 0.9011 - val_recall: 0.8977\nEpoch 72/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0049 - binary_crossentropy: 0.0049 - precision: 0.9972 - recall: 0.9943 - val_loss: 0.1280 - val_binary_crossentropy: 0.1280 - val_precision: 0.8966 - val_recall: 0.8864\nEpoch 73/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0109 - binary_crossentropy: 0.0109 - precision: 0.9886 - recall: 0.9886 - val_loss: 0.1360 - val_binary_crossentropy: 0.1360 - val_precision: 0.8783 - val_recall: 0.8750\nEpoch 74/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0109 - binary_crossentropy: 0.0109 - precision: 0.9895 - recall: 0.9858 - val_loss: 0.1235 - val_binary_crossentropy: 0.1235 - val_precision: 0.9087 - val_recall: 0.8674\nEpoch 75/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0058 - binary_crossentropy: 0.0058 - precision: 0.9971 - recall: 0.9915 - val_loss: 0.1001 - val_binary_crossentropy: 0.1001 - val_precision: 0.9213 - val_recall: 0.8864\nEpoch 76/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0030 - binary_crossentropy: 0.0030 - precision: 0.9972 - recall: 0.9991 - val_loss: 0.1072 - val_binary_crossentropy: 0.1072 - val_precision: 0.9011 - val_recall: 0.8977\nEpoch 77/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0018 - binary_crossentropy: 0.0018 - precision: 0.9991 - recall: 0.9972 - val_loss: 0.1121 - val_binary_crossentropy: 0.1121 - val_precision: 0.9306 - val_recall: 0.8636\nEpoch 78/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0061 - binary_crossentropy: 0.0061 - precision: 0.9943 - recall: 0.9934 - val_loss: 0.1046 - val_binary_crossentropy: 0.1046 - val_precision: 0.9237 - val_recall: 0.8712\nEpoch 79/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0023 - binary_crossentropy: 0.0023 - precision: 0.9972 - recall: 0.9981 - val_loss: 0.1019 - val_binary_crossentropy: 0.1019 - val_precision: 0.9209 - val_recall: 0.8826\nEpoch 80/100\n226/226 [==============================] - 5s 22ms/step - loss: 0.0071 - binary_crossentropy: 0.0071 - precision: 0.9924 - recall: 0.9924 - val_loss: 0.0980 - val_binary_crossentropy: 0.0980 - val_precision: 0.9112 - val_recall: 0.8939\nEpoch 81/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0124 - binary_crossentropy: 0.0124 - precision: 0.9904 - recall: 0.9782 - val_loss: 0.1417 - val_binary_crossentropy: 0.1417 - val_precision: 0.8473 - val_recall: 0.8826\nEpoch 82/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0096 - binary_crossentropy: 0.0096 - precision: 0.9848 - recall: 0.9830 - val_loss: 0.1073 - val_binary_crossentropy: 0.1073 - val_precision: 0.8835 - val_recall: 0.8902\nEpoch 83/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0108 - binary_crossentropy: 0.0108 - precision: 0.9858 - recall: 0.9848 - val_loss: 0.1086 - val_binary_crossentropy: 0.1086 - val_precision: 0.8937 - val_recall: 0.8598\nEpoch 84/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0089 - binary_crossentropy: 0.0089 - precision: 0.9896 - recall: 0.9877 - val_loss: 0.1072 - val_binary_crossentropy: 0.1072 - val_precision: 0.8911 - val_recall: 0.8674\nEpoch 85/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0049 - binary_crossentropy: 0.0049 - precision: 0.9962 - recall: 0.9962 - val_loss: 0.0998 - val_binary_crossentropy: 0.0998 - val_precision: 0.9141 - val_recall: 0.8864\nEpoch 86/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0015 - binary_crossentropy: 0.0015 - precision: 0.9981 - recall: 0.9981 - val_loss: 0.1035 - val_binary_crossentropy: 0.1035 - val_precision: 0.9183 - val_recall: 0.8939\nEpoch 87/100\n226/226 [==============================] - 5s 21ms/step - loss: 9.1433e-04 - binary_crossentropy: 9.1433e-04 - precision: 1.0000 - recall: 0.9991 - val_loss: 0.1062 - val_binary_crossentropy: 0.1062 - val_precision: 0.9173 - val_recall: 0.8826\nEpoch 88/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0014 - binary_crossentropy: 0.0014 - precision: 0.9991 - recall: 0.9981 - val_loss: 0.1090 - val_binary_crossentropy: 0.1090 - val_precision: 0.9027 - val_recall: 0.8788\nEpoch 89/100\n226/226 [==============================] - 5s 21ms/step - loss: 9.1919e-04 - binary_crossentropy: 9.1919e-04 - precision: 0.9981 - recall: 0.9991 - val_loss: 0.1071 - val_binary_crossentropy: 0.1071 - val_precision: 0.9206 - val_recall: 0.8788\nEpoch 90/100\n226/226 [==============================] - 5s 21ms/step - loss: 4.5876e-04 - binary_crossentropy: 4.5876e-04 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.1159 - val_binary_crossentropy: 0.1159 - val_precision: 0.9312 - val_recall: 0.8712\nEpoch 91/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0061 - binary_crossentropy: 0.0061 - precision: 0.9934 - recall: 0.9905 - val_loss: 0.1039 - val_binary_crossentropy: 0.1039 - val_precision: 0.9127 - val_recall: 0.8712\nEpoch 92/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0021 - binary_crossentropy: 0.0021 - precision: 0.9981 - recall: 0.9972 - val_loss: 0.1105 - val_binary_crossentropy: 0.1105 - val_precision: 0.9228 - val_recall: 0.8598\nEpoch 93/100\n226/226 [==============================] - 5s 21ms/step - loss: 6.4699e-04 - binary_crossentropy: 6.4699e-04 - precision: 0.9991 - recall: 1.0000 - val_loss: 0.1115 - val_binary_crossentropy: 0.1115 - val_precision: 0.9124 - val_recall: 0.8674\nEpoch 94/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0018 - binary_crossentropy: 0.0018 - precision: 0.9991 - recall: 0.9981 - val_loss: 0.1235 - val_binary_crossentropy: 0.1235 - val_precision: 0.8833 - val_recall: 0.8598\nEpoch 95/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0126 - binary_crossentropy: 0.0126 - precision: 0.9896 - recall: 0.9867 - val_loss: 0.1405 - val_binary_crossentropy: 0.1405 - val_precision: 0.8697 - val_recall: 0.8598\nEpoch 96/100\n226/226 [==============================] - 5s 20ms/step - loss: 0.0151 - binary_crossentropy: 0.0151 - precision: 0.9829 - recall: 0.9811 - val_loss: 0.1126 - val_binary_crossentropy: 0.1126 - val_precision: 0.9051 - val_recall: 0.8674\nEpoch 97/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0035 - binary_crossentropy: 0.0035 - precision: 0.9972 - recall: 0.9953 - val_loss: 0.1063 - val_binary_crossentropy: 0.1063 - val_precision: 0.9105 - val_recall: 0.8864\nEpoch 98/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0016 - binary_crossentropy: 0.0016 - precision: 0.9981 - recall: 0.9991 - val_loss: 0.0960 - val_binary_crossentropy: 0.0960 - val_precision: 0.9141 - val_recall: 0.8864\nEpoch 99/100\n226/226 [==============================] - 5s 21ms/step - loss: 8.5929e-04 - binary_crossentropy: 8.5929e-04 - precision: 1.0000 - recall: 0.9991 - val_loss: 0.1039 - val_binary_crossentropy: 0.1039 - val_precision: 0.9203 - val_recall: 0.8750\nEpoch 100/100\n226/226 [==============================] - 5s 21ms/step - loss: 0.0030 - binary_crossentropy: 0.0030 - precision: 0.9981 - recall: 0.9962 - val_loss: 0.1051 - val_binary_crossentropy: 0.1051 - val_precision: 0.9102 - val_recall: 0.8826\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7ePT1WMRNc1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b970161-63ae-484d-b53c-c4bbefe220e4"
      },
      "source": [
        "patient = '9578'\n",
        "k = list(history.history.keys())\n",
        "precision_train = history.history[k[2]][-1]\n",
        "recall_train = history.history[k[3]][-1]\n",
        "precision_test = history.history[k[6]][-1]\n",
        "recall_test = history.history[k[7]][-1]\n",
        "\n",
        "try:\n",
        "  f1_train = 2 * ( (precision_train * recall_train) / (precision_train + recall_train) )\n",
        "  f1_test = 2 * ( (precision_test * recall_test) / (precision_test + recall_test) )\n",
        "except ZeroDivisionError:\n",
        "  f1_train = 0\n",
        "  f1_test = 0\n",
        "\n",
        "print(f1_train, f1_test)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0.9971564095657722 0.8961538424294375\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MelEJ9MSLDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_preds_train = model.predict_classes(X_train_resh)\n",
        "y_preds_test = model.predict_classes(X_test_resh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BQtEjwxS78J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNUIXx5dTC9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3bb2ebfc-cd4f-473b-8b74-2edbb4aad7c1"
      },
      "source": [
        "f1_score(Y_test, y_preds_test)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.38575667655786355"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oH-2jAQ607v",
        "colab_type": "code",
        "outputId": "948560f1-27cd-4537-f62a-ff21c312112b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "all_patients = {}\n",
        "count = 0\n",
        "for i in list(patients)[27:]:\n",
        "  print(count)\n",
        "  all_df = {}\n",
        "  count += 1\n",
        "  for df in zip(dict_df, [0, 1, 3, 6, 12, 24]):\n",
        "    df_f = dict_df[df][dict_df[df].Key.str.contains(i)]\n",
        "    X = df_f.iloc[:, 1:-1].values\n",
        "    Y = df_f.iloc[:, -1:].values\n",
        "\n",
        "    X_train_fit, X_test_fit, Y_train, Y_test = train_test_split(X, Y, random_state=42, stratify=Y, train_size=0.8)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_fit = scaler.fit_transform(X_train_fit)\n",
        "    X_test_fit = scaler.transform(X_test_fit)\n",
        "\n",
        "    nb_period = 0\n",
        "\n",
        "    #reshape specifique en 3D pour le format attendu pour le LSTM\n",
        "    param2 = int(X_train_fit.shape[1]/(nb_period+1))\n",
        "    X_train_resh = X_train_fit.reshape((X_train_fit.shape[0],nb_period+1,param2))\n",
        "    X_test_resh = X_test_fit.reshape((X_test_fit.shape[0], nb_period+1, param2))\n",
        "    #print(X_train_resh.shape, Y_train.shape, X_test_resh.shape, Y_test.shape)\n",
        "\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.LSTM(600, input_shape=(X_train_resh.shape[1], X_train_resh.shape[2])))\n",
        "    model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "    # Créons un learning rate schedule pour décroitre le learning rate à mesure que nous entrainons le modèle \n",
        "    initial_learning_rate = 0.001\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate,\n",
        "        decay_steps=1090,\n",
        "        decay_rate=0.96,\n",
        "        staircase=True)\n",
        "    # Utilisation d'un compileur simple avec un optimiseur Adam pour le calcul de nos gradients \n",
        "    optimizer= tf.keras.optimizers.Adam(\n",
        "        learning_rate = lr_schedule\n",
        "    )\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                  metrics=[tf.keras.metrics.BinaryCrossentropy(),\n",
        "                          tf.keras.metrics.Precision(),\n",
        "                          tf.keras.metrics.Recall()])\n",
        "\n",
        "    history = model.fit(X_train_resh, Y_train, epochs=100, batch_size=36, validation_data=(X_test_resh, Y_test), verbose=0, shuffle=False)\n",
        "\n",
        "    y_preds_train = model.predict_classes(X_train_resh)\n",
        "    y_preds_test = model.predict_classes(X_test_resh)\n",
        "\n",
        "    cm_train = confusion_matrix(Y_train, y_pred_train)\n",
        "    cm_test = confusion_matrix(Y_test, y_pred_test)\n",
        "\n",
        "    try:\n",
        "      score = {\n",
        "        'f1_score train': f1_score(y_train, y_pred_train),\n",
        "        'f1_score test': f1_score(y_test, y_pred_test),\n",
        "        'cm train': cm_train,\n",
        "        'cm test': cm_test,\n",
        "        'sensitivity train': cm_train[1][1] / (cm_train[1][0] + cm_train[1][1]),\n",
        "        'sensitivity test': cm_test[1][1] / (cm_test[1][0] + cm_test[1][1]),\n",
        "        'specificity train': cm_train[0][0] / (cm_train[0][0] + cm_train[0][1]),\n",
        "        'specificity test': cm_test[0][0] / (cm_test[0][0] + cm_test[0][1])\n",
        "      }\n",
        "    except IndexError:\n",
        "      score = {\n",
        "        'f1_score train': f1_score(Y_train, y_pred_train),\n",
        "        'f1_score test': f1_score(Y_test, y_pred_test),\n",
        "        'cm train': cm_train,\n",
        "        'cm test': cm_test\n",
        "      }\n",
        "\n",
        "    all_df[df] = score\n",
        "\n",
        "  all_patients[i] = all_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGQUGxiBDYhG",
        "colab_type": "code",
        "outputId": "88f7c6e4-dec0-430f-89b0-4cc315c887e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "all_patients['9885']['10'].history.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-be8f13e494ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_patients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'9885'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'10'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: '9885'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzqnoNybOjx9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "6ebe1148-1a69-42b5-a201-4f6340c8876c"
      },
      "source": [
        "patient = '9578'\n",
        "k = list(all_patients[patient]['60'].history.keys())\n",
        "precision_train = all_patients[patient]['60'].history[k[2]][-1]\n",
        "recall_train = all_patients[patient]['60'].history[k[3]][-1]\n",
        "precision_test = all_patients[patient]['60'].history[k[6]][-1]\n",
        "recall_test = all_patients[patient]['60'].history[k[7]][-1]\n",
        "\n",
        "try:\n",
        "  f1_train = 2 * ( (precision_train * recall_train) / (precision_train + recall_train) )\n",
        "  f1_test = 2 * ( (precision_test * recall_test) / (precision_test + recall_test) )\n",
        "except ZeroDivisionError:\n",
        "  f1_train = 0\n",
        "  f1_test = 0\n",
        "\n",
        "f1_train\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-80d654fdc99a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpatient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'9578'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_patients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpatient\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'60'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprecision_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_patients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpatient\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'60'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrecall_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_patients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpatient\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'60'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprecision_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_patients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpatient\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'60'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'all_patients' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2tWSKzeNqKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = pd.DataFrame()\n",
        "for i in list(patients)[27:]:\n",
        "  for z in all_patients[i]:\n",
        "\n",
        "    k = list(all_patients[i][z].history.keys())\n",
        "    precision_train = all_patients[i][z].history[k[2]][-1]\n",
        "    recall_train = all_patients[i][z].history[k[3]][-1]\n",
        "    precision_test = all_patients[i][z].history[k[6]][-1]\n",
        "    recall_test = all_patients[i][z].history[k[7]][-1]\n",
        "\n",
        "    try:\n",
        "      f1_train = 2 * ( (precision_train * recall_train) / (precision_train + recall_train) )\n",
        "    except ZeroDivisionError:\n",
        "      f1_train = 0\n",
        "\n",
        "    try:\n",
        "      f1_test = 2 * ( (precision_test * recall_test) / (precision_test + recall_test) )\n",
        "    except ZeroDivisionError:\n",
        "      f1_test = 0\n",
        "\n",
        "    d = d.append(pd.DataFrame({\n",
        "        'Patient': i,\n",
        "        'tempo': z,\n",
        "        'f1 score train': f1_train,\n",
        "        'f1 score test': f1_test,\n",
        "        'precision train': precision_train,\n",
        "        'precision test': precision_test,\n",
        "        'recall train': recall_train,\n",
        "        'recall test': recall_test\n",
        "        \n",
        "    }, index=[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CM7K-CWOCXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d.to_csv('/content/LSTM_26_first_patients.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6r17qnbSScZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}